{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spark_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "4JRsdwQBrK9Q"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "IEv-xC7SrXPk"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "ziSzm8YxrbSw"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHnaKL83Hpmu",
        "outputId": "3a175fa8-6c22-4c66-c22a-953d1ee959d5"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example inp and out\n",
        "# s3://comp4442-group-project/data/\n",
        "# s3://comp4442-group-project/output\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "from pyspark import SparkContext, SQLContext, Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# args = sys.argv\n",
        "# inp = args[1]\n",
        "# out = args[2]\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "inputList = []\n",
        "for i in range(2,12):\n",
        "  index = 0\n",
        "  if i < 10:\n",
        "    index = \"0\" + str(i)\n",
        "  else:\n",
        "    index = i\n",
        "\n",
        "  inputList.append(f\"/content/drive/MyDrive/csvdata/detail_record_2017_01_{index}_08_00_00\")\n",
        "\n",
        "# text_file = sc.textFile(inp)\n",
        "print(inputList)\n",
        "\n",
        "\n",
        "\n",
        "            "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gX0g1BxFTuj",
        "outputId": "a80f2f11-1fae-42d8-f7b3-b039b98006f3"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/csvdata/detail_record_2017_01_02_08_00_00', '/content/drive/MyDrive/csvdata/detail_record_2017_01_03_08_00_00', '/content/drive/MyDrive/csvdata/detail_record_2017_01_04_08_00_00', '/content/drive/MyDrive/csvdata/detail_record_2017_01_05_08_00_00', '/content/drive/MyDrive/csvdata/detail_record_2017_01_06_08_00_00', '/content/drive/MyDrive/csvdata/detail_record_2017_01_07_08_00_00', '/content/drive/MyDrive/csvdata/detail_record_2017_01_08_08_00_00', '/content/drive/MyDrive/csvdata/detail_record_2017_01_09_08_00_00', '/content/drive/MyDrive/csvdata/detail_record_2017_01_10_08_00_00', '/content/drive/MyDrive/csvdata/detail_record_2017_01_11_08_00_00']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i ,input in enumerate(inputList):\n",
        "  text_file = sc.textFile(input)\n",
        "  # split each line into a list of fields\n",
        "  # only count the lines that have at least 9 fields, ignore the case that is no special behavior\n",
        "  counts = text_file.map(lambda line: line.split(\",\")).filter(lambda line: len(line)>8)\n",
        "\n",
        "\n",
        "  column_data = counts.map(lambda p: Row(p[0], p[1], p[2], p[3], p[4], \\\n",
        "                                      p[5], p[6], p[7], p[8], p[9] , \\\n",
        "                                      p[10], p[11], p[12], p[13], p[14], \\\n",
        "                                      p[15], p[16], p[17], p[18]))\n",
        "\n",
        "\n",
        "  column_name = \"driverID,carPlateNumber,Latitude,Longitude,Speed,Direction,siteName,Time,isRapidlySpeedup,isRapidlySlowdown,isNeutralSlide,isNeutralSlideFinished,neutralSlideTime,isOverspeed,isOverspeedFinished,overspeedTime,isFatigueDriving,isHthrottleStop,isOilLeak\"\n",
        "  sql = \"SELECT first(recordID),first(driverID),first(carPlateNumber),first(Time) \\\n",
        "                              as recordDAY,HOUR(Time) as recordHOUR,\\\n",
        "                              sum(isRapidlySpeedup),sum(isRapidlySlowdown),sum(isNeutralSlide),sum(isNeutralSlideFinished),\\\n",
        "                              sum(neutralSlideTime),sum(isOverspeed),sum(isOverspeedFinished),sum(overspeedTime),sum(isFatigueDriving),\\\n",
        "                              sum(isHthrottleStop),sum(isOilLeak) \\\n",
        "                              FROM summary \\\n",
        "                              GROUP BY driverID,DAY(Time),HOUR(Time)\"\n",
        "\n",
        "  # create schema with the column names\n",
        "  fields = [StructField(field_name, StringType(), True) for field_name in column_name.split(\",\")]\n",
        "  schema = StructType(fields)\n",
        "\n",
        "  # apply the schema to the RDD\n",
        "  dataframe = sqlContext.createDataFrame(column_data,schema)\n",
        "  # add the recordID to store where these record come from\n",
        "  dataframe = dataframe.withColumn(\"recordID\",lit(input))\n",
        "  # register the DataFrame as a table.\n",
        "  dataframe.registerTempTable(\"summary\")\n",
        "\n",
        "  # execute the SQL query and save the result to the output directory\n",
        "  group_data = sqlContext.sql(sql)\n",
        "  group_data.coalesce(1).write.csv(\"/content/drive/MyDrive/csvdata/data\"+str(i))\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "H0oVE4HIFbw7"
      },
      "execution_count": 130,
      "outputs": []
    }
  ]
}